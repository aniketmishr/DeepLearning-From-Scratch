{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq8toX9cgkmu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class InputEmbedding(nn.Module):\n",
        "    def __init__(self, d_model:int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
        "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
        "        # x shape : (Batch, seq)\n",
        "        # return : (Batch, seq, dim)\n",
        "        return self.embedding(x) * (self.d_model ** 0.5)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Create a matrix of shape (seq_len, d_model)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        # Create a vector of shape (seq_len)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
        "        # Create a vector of shape (d_model)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
        "        # Apply sine to even indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
        "        # Apply cosine to odd indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
        "        # Add a batch dimension to the positional encoding\n",
        "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
        "        # Register the positional encoding as a buffer\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, eps = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(1)) # multiplied\n",
        "        self.beta = nn.Parameter(torch.zeros(1)) # added\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdims=True)\n",
        "        std = x.std(dim=-1, keepdims=True)\n",
        "        return self.alpha* (x-mean)/(std+self.eps) + self.beta\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff:int, dropout:float):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff) # (batch, seq_len, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model) # (batch, seq_len, d_model)\n",
        "    def forward(self,x):\n",
        "        # x shape: (batch, seq_len, d_model)\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
        "\n",
        "class MultiHeadedAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model:int, h:int, dropout: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        assert d_model % h ==0 , \"d_model is not divisible by h\"\n",
        "        self.d_k = d_model // h\n",
        "        self.w_q = nn.Linear(d_model,d_model,bias=False)\n",
        "        self.w_k = nn.Linear(d_model,d_model,bias=False)\n",
        "        self.w_v = nn.Linear(d_model,d_model,bias=False)\n",
        "\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key,value, mask, dropout:nn.Dropout) :\n",
        "        d_k = query.shape[-1]\n",
        "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "        attention_scores = (query @ key.transpose(-2,-1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            attention_scores.masked_fill_(mask==0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim=-1)\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        return (attention_scores @ value) , attention_scores\n",
        "\n",
        "    def forward(self,q,k,v,mask):\n",
        "        # x shape : (batch , seq_len, d_model)\n",
        "        # return : (batch , seq_len, d_model)\n",
        "\n",
        "        query = self.w_q(q)     # (batch , seq_len, d_model)\n",
        "        key = self.w_k(k)       # (batch , seq_len, d_model)\n",
        "        value = self.w_v(v)     # (batch , seq_len, d_model)\n",
        "        # (batch , seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        query = query.view(query.shape[0], query.shape[1],self.h, self.d_k).transpose(1,2)\n",
        "        key = key.view(key.shape[0], key.shape[1],self.h, self.d_k).transpose(1,2)\n",
        "        value = value.view(value.shape[0], value.shape[1],self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "        x, attention_scores = MultiHeadedAttentionBlock.attention(query, key, value,mask, self.dropout)\n",
        "\n",
        "        # Combine all the head together\n",
        "        x = x.transpose(1,2).contiguous().flatten(2) # (batch, seq_len, h*d_k) --> (batch, seq_len, d_model)\n",
        "        # Multiply by Wo\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, dropout: float):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalization()\n",
        "    def forward(self,x, sublayer):\n",
        "        out = x + self.dropout(sublayer(self.norm(x)))  # Skip connections\n",
        "        return out\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention_block: MultiHeadedAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "    def forward(self,x, src_mask):\n",
        "        x = self.residual_connection[0](x, lambda x: self.self_attention_block(x,x,x,src_mask))\n",
        "        x = self.residual_connection[1](x, self.feed_forward_block)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x,mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention_block: MultiHeadedAttentionBlock, cross_attention_block: MultiHeadedAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
        "    def forward(self, x, encoder_output, src_mask, target_mask):\n",
        "        # x shape: (batch, seq_len, d_model)\n",
        "        # encoder_output : (batch, seq_len, d_model)\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x,x,x,target_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "    def forward(self, x, encoder_output, src_mask, target_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, target_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, seq_len, d_model)\n",
        "        # output: (batch, seq_lem , vocab_size)\n",
        "        return torch.log_softmax(self.proj(x),dim=-1)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbedding, tgt_embed: InputEmbedding, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        # (batch, seq_len, d_model)\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self,encoder_output, src_mask, tgt, target_mask ):\n",
        "        # (batch, seq_len, d_model)\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, target_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        # (batch, seq_len, d_model)\n",
        "        return self.projection_layer(x)\n",
        "\n",
        "def build_transformer(src_vocab_size, tgt_vocab_size, src_seq_len, tgt_seq_len, d_model = 512, N=6, h= 8, dropout= 0.3, d_ff = 2048):\n",
        "\n",
        "    # create the embedding layers\n",
        "    src_embed = InputEmbedding(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbedding(d_model, tgt_vocab_size)\n",
        "    # create the positional encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadedAttentionBlock(d_model, h, dropout)\n",
        "        encoder_feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(encoder_self_attention_block, encoder_feed_forward_block,dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadedAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadedAttentionBlock(d_model, h, dropout)\n",
        "        decoder_feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(decoder_self_attention_block , decoder_cross_attention_block, decoder_feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # create the encoder and decoder\n",
        "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def get_config():\n",
        "    return {\n",
        "        \"batch_size\": 16,\n",
        "        \"num_epochs\": 20,\n",
        "        \"lr\": 10**-4,\n",
        "        \"seq_len\": 150,\n",
        "        \"d_model\": 512,\n",
        "        # \"datasource\": 'opus_books',\n",
        "        \"lang_src\": \"en\",\n",
        "        \"lang_tgt\": \"hi\",\n",
        "        \"model_folder\": \"weights\",\n",
        "        \"model_basename\": \"tmodel_\",\n",
        "        \"preload\": None,\n",
        "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
        "        \"experiment_name\": \"runs/tmodel\"\n",
        "    }\n",
        "\n",
        "def get_weights_file_path(config, epoch:str):\n",
        "    model_folder = config['model_folder']\n",
        "    model_basename = config['model_basename']\n",
        "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
        "    return str(Path('.')/model_folder/model_filename)"
      ],
      "metadata": {
        "id": "acIGr4yjJjqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BilingualDataset(Dataset):\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "        super().__init__()\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "        self.seq_len = seq_len\n",
        "        # create special token tensors\n",
        "        self.sos_token = torch.Tensor([tokenizer_src.token_to_id(\"[SOS]\")]).type(torch.int64)\n",
        "        self.eos_token = torch.Tensor([tokenizer_src.token_to_id(\"[EOS]\")]).type(torch.int64)\n",
        "        self.pad_token = torch.Tensor([tokenizer_src.token_to_id(\"[PAD]\")]).type(torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_target_pair = self.ds[idx]\n",
        "        src_text = src_target_pair['translation'][self.src_lang]\n",
        "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        # Transform the text into tokens\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        # Add sos , eos and padding to each sentence\n",
        "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # we will add [SOS] and [EOS] to the sequence\n",
        "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # We will add only [SOS] to the sequence\n",
        "\n",
        "        if enc_num_padding_tokens<0 or dec_num_padding_tokens<0:\n",
        "            raise ValueError(\"Sentence too long\")\n",
        "\n",
        "        # Encoder Input: Add [SOS], [EOS] and [PAD]\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token]*enc_num_padding_tokens, dtype = torch.int64),\n",
        "            ],\n",
        "            dim = 0\n",
        "        )\n",
        "\n",
        "        # Decoder Input : Add [SOS] and [PAD]\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens,dtype=torch.int64),\n",
        "                torch.tensor([self.pad_token]*dec_num_padding_tokens, dtype = torch.int64)\n",
        "            ],\n",
        "            dim= 0\n",
        "        )\n",
        "\n",
        "        # Label: Add [EOS] and [PAD]\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens,dtype = torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token]*dec_num_padding_tokens, dtype = torch.int64)\n",
        "            ],\n",
        "            dim= 0\n",
        "        )\n",
        "\n",
        "        # Double check the size of the tensors to make sure they are all seq_len long\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input, # (seq_len)\n",
        "            \"decoder_input\": decoder_input, # (seq_len)\n",
        "            \"encoder_mask\": (encoder_input!=self.pad_token).unsqueeze(0).unsqueeze(0).int(), # During self attention, we don't want our [PAD] tokens to interact with each other # (1,1,seq_len)\n",
        "            \"decoder_mask\": (decoder_input!=self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),  # padding tokens shouldn't interact and tokens should only interact with the tokens that comes prior to it # (1,seq_len,seq_len)\n",
        "            \"label\" : label,  # (seq_len)\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text,\n",
        "        }\n",
        "\n",
        "def casual_mask(size):\n",
        "    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int) # (size, size) --> (seq_len, seq_len)\n",
        "    return mask == 0"
      ],
      "metadata": {
        "id": "6vey7AtJJmdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# from dataset import BilingualDataset, casual_mask\n",
        "# from model import build_transformer\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "# importing necessary hugging face libraries\n",
        "# !pip install datasets\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from pathlib import Path\n",
        "\n",
        "# from config import get_weights_file_path, get_config\n",
        "\n",
        "def get_all_sentences(ds,lang):\n",
        "    for item in ds:\n",
        "        yield item[\"translation\"][lang]\n",
        "\n",
        "def get_or_build_tokenizer(config, ds, lang:str):\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\",\"[PAD]\",\"[SOS]\",\"[EOS]\"], min_frequency=2)\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang),trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer\n",
        "\n",
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    sos_idx = tokenizer_src.token_to_id(\"[SOS]\")\n",
        "    eos_idx = tokenizer_src.token_to_id(\"[EOS]\")\n",
        "    # Precomputes the encoder output and resuse it for every token we get from the decoder\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    # Initialize the decoder input the SOS token\n",
        "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
        "    while True:\n",
        "        if decoder_input.size(1)==max_len:\n",
        "            break\n",
        "        # Build Mask for the target (decoder input)\n",
        "        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "        # calculate the output of the decoder\n",
        "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "        # Get the next token\n",
        "        prob = model.project(out[:,-1])\n",
        "        # select the token with the max prob as this is the greedy search!\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat([decoder_input, torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
        "\n",
        "        if next_word== eos_idx:\n",
        "            break\n",
        "    return decoder_input.squeeze(0)\n",
        "\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples = 2):\n",
        "    model.eval()\n",
        "    count=0\n",
        "\n",
        "    # source_texts = []\n",
        "    # expected = []\n",
        "    # predicted = []\n",
        "    # size of the control window\n",
        "    console_width = 80\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count+=1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device)\n",
        "\n",
        "            assert encoder_input.size(0)==1 ,\"Batch size must be 1 for validation\"\n",
        "\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"tgt_text\"][0]\n",
        "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "            # source_texts.append(source_text)\n",
        "            # expected.append(target_text)\n",
        "            # predicted.append(model_out_text)\n",
        "\n",
        "            # print the msg to console\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f\"SOURCE: {source_text}\")\n",
        "            print_msg(f\"TARGET: {target_text}\")\n",
        "            print_msg(f\"PREDICTED: {model_out_text}\")\n",
        "\n",
        "            if count==num_examples:\n",
        "                break\n",
        "\n",
        "\n",
        "def get_ds(config):\n",
        "    ds_raw = load_dataset(\"cfilt/iitb-english-hindi\" ,split = 'train') #, f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split='train')\n",
        "    ds_raw = ds_raw.select(range(20000))\n",
        "    # Build tokenizers\n",
        "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])\n",
        "    # Train-val split : 90 % for train, 10% for val\n",
        "    train_ds_size = int(0.9*len(ds_raw))\n",
        "    val_ds_size = len(ds_raw)-train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "    # Find the maximum length of each sentence in the source and target sentence\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "\n",
        "    for item in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {max_len_src}')\n",
        "    print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "    val_dataloader =  DataLoader(val_ds,batch_size=1, shuffle=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
        "\n",
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "    model = build_transformer(vocab_src_len,vocab_tgt_len,config['seq_len'],config['seq_len'])\n",
        "    return model\n",
        "\n",
        "def train_model(config):\n",
        "    # define the device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'On Device: {device}')\n",
        "\n",
        "    # make sure the weights folder exists\n",
        "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "    # Tensorboard\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = config['lr'], eps=1e-9)\n",
        "\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "    if config['preload']:\n",
        "        model_filename = get_weights_file_path(config, config['preload'])\n",
        "        print(f\"Preloading Model {model_filename}\")\n",
        "        state = torch.load(model_filename)\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id(\"[PAD]\"), label_smoothing=0.1).to(device)\n",
        "    print(\"Training the model: \")\n",
        "    for epoch in range(initial_epoch, initial_epoch + config['num_epochs']):\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "        for batch in batch_iterator:\n",
        "            model.train()\n",
        "            encoder_input = batch[\"encoder_input\"].to(device) # (B, seq_len)\n",
        "            decoder_input = batch[\"decoder_input\"].to(device) # (B, seq_len)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device)   # (B,1, seq_len)\n",
        "            decoder_mask = batch[\"decoder_mask\"].to(device)   # (B,seq_len, seq_len)\n",
        "\n",
        "            # run the tensors through the transformer\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
        "            decoder_output = model.decode(encoder_output,encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
        "            proj_output = model.project(decoder_output) # (B, seq_len, target_vocab_size)\n",
        "\n",
        "            label = batch[\"label\"].to(device) # (B, seq_len)\n",
        "\n",
        "            # (B,seq_len, tgt_vocab_size) --> (B*seq_len, tgt_vocab_size)\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            # Log the loss on tensorboard\n",
        "            writer.add_scalar(\"train loss\", loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            # backpropagtes the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # update the weights\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config[\"seq_len\"], device, lambda msg: batch_iterator.write(msg) ,global_step, writer)\n",
        "\n",
        "            global_step+=1\n",
        "\n",
        "        # Run validation after epoch to see how the model is performing\n",
        "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config[\"seq_len\"], device, lambda msg: batch_iterator.write(msg) ,global_step, writer)\n",
        "\n",
        "        # save the model at the end of every epoch\n",
        "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step\n",
        "        }, model_filename)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    warnings.filterwarnings('ignore')\n",
        "    config = get_config()\n",
        "    train_model(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKZfjLkbJdT2",
        "outputId": "b744c376-3e6f-4726-f77d-6f96c8c3bd2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On Device: cuda\n",
            "Max length of source sentence: 70\n",
            "Max length of target sentence: 98\n",
            "Training the model: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 00: 100%|██████████| 1125/1125 [04:58<00:00,  3.77it/s, loss=4.536]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Show right margin\n",
            "TARGET: दिखाएँ दायाँ\n",
            "PREDICTED: प्रोजेक्ट फ़ाइल\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: _ Select\n",
            "TARGET: चुनें (_ S) \n",
            "PREDICTED: ( _ S )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 01: 100%|██████████| 1125/1125 [04:58<00:00,  3.77it/s, loss=4.001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Use pkg - config to add library support from other packages\n",
            "TARGET: उपयोग को जोड़ें समर्थन से अन्य संकुल\n",
            "PREDICTED: को से फ़ाइल से . को को नहीं से नहीं को को फ़ाइल से .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Selected accessible\n",
            "TARGET: चुने गए एक्सेसेबेल\n",
            "PREDICTED: चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित चयनित\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 02: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s, loss=2.671]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Move a card or build of cards on to the empty slot\n",
            "TARGET: एक पत्ता या पत्तों के समूह खाली खाँचा पर ले जाएँ\n",
            "PREDICTED: ~ a को एक खाली तस्वीर खाँचा पर ले जाएँ .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Command\n",
            "TARGET: कमांड\n",
            "PREDICTED: कमांड\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 03: 100%|██████████| 1125/1125 [04:58<00:00,  3.77it/s, loss=2.531]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: C compiler flags:\n",
            "TARGET: सी फ्लैग्सः\n",
            "PREDICTED: सी फ्लैग्सः\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: % s has no text interface\n",
            "TARGET: % s का कोई पाठ अंतराफलक नहीं है\n",
            "PREDICTED: % s का कोई नहीं है से नहीं से . नहीं से . नहीं रिमोट रिमोट रिमोट से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं रिमोट रिमोट रिमोट रिमोट रिमोट से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से रिमोट से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से . नहीं से .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 04: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s, loss=2.271]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Generate patches relative to:\n",
            "TARGET: बनाएँ (G) कोः\n",
            "PREDICTED: नया निर्देशिका कोः\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Place the four of spades next to the three of spades.\n",
            "TARGET: हुकुम की दुक्का के बगल में हुकुम के तिक्की को रखें. \n",
            "PREDICTED: हुकुम की हुकुम के हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम की हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम की हुकुम हुकुम हुकुम हुकुम की हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम के हुकुम की हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम की हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम की हुकुम की हुकुम हुकुम हुकुम की हुकुम की हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम की हुकुम की हुकुम की हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम की हुकुम की हुकुम की हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम हुकुम\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 05: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s, loss=2.014]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Skip\n",
            "TARGET: छोड़ें\n",
            "PREDICTED: छोड़ें\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Base Card: ~ a\n",
            "TARGET: आधार पत्ताः ~ a\n",
            "PREDICTED: आधार पत्ताः ~ a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 06: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s, loss=2.480]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Ignore Files:\n",
            "TARGET: लछ्य फाइल\n",
            "PREDICTED: फ़ाइल निर्देशिका\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: (error)\n",
            "TARGET: (त्रुटि) \n",
            "PREDICTED: ( त्रुटि )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 07: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s, loss=1.834]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: The shortcut is already used by another component in Anjuta. Do you want to keep it anyway?\n",
            "TARGET: शॉर्टकट है प्रयुक्त द्वारा इंच Anjuta को? \n",
            "PREDICTED: Anjuta है इंच प्रयोक्ता Anjuta है इंच Anjuta प्रयोक्ता Anjuta प्रयोक्ता Anjuta प्रयोक्ता को इंच Anjuta प्रयोक्ता Anjuta प्रयोक्ता इंच Anjuta प्रयोक्ता इंच Anjuta है इंच Anjuta इंच Anjuta प्रयोक्ता प्रयोक्ता प्रयोक्ता है इंच Anjuta इंच Anjuta इंच Anjuta इंच Anjuta इंच Anjuta इंच Anjuta इंच Anjuta इंच Anjuta इंच Anjuta इंच प्रयोक्ता इंच प्रयोक्ता प्रयोक्ता को प्रयोक्ता को इंच Anjuta प्रयोक्ता प्रयोक्ता इंच Anjuta प्रयोक्ता इंच Anjuta प्रयोक्ता इंच Anjuta इंच Anjuta इंच Anjuta इंच Anjuta इंच Anjuta इंच Anjuta प्रयोक्ता इंच Anjuta प्रयोक्ता Anjuta प्रयोक्ता Anjuta प्रयोक्ता इंच Anjuta प्रयोक्ता इंच Anjuta प्रयोक्ता प्रयोक्ता प्रयोक्ता इंच Anjuta प्रयोक्ता इंच प्रयोक्ता इंच प्रयोक्ता इंच Anjuta प्रयोक्ता इंच Anjuta प्रयोक्ता प्रयोक्ता प्रयोक्ता प्रयोक्ता प्रयोक्ता प्रयोक्ता इंच Anjuta प्रयोक्ता इंच Anjuta इंच Anjuta प्रयोक्ता इंच Anjuta प्रयोक्ता कमांड इंच Anjuta प्रयोक्ता Anjuta प्रयोक्ता कमांड इंच Anjuta प्रयोक्ता इंच प्रयोक्ता इंच प्रयोक्ता इंच Anjuta प्रयोक्ता कमांड इंच Anjuta प्रयोक्ता इंच Anjuta\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Repository to push to:\n",
            "TARGET: रेपोसिटरी को दबाएं कोः\n",
            "PREDICTED: रेपोसिटरी को कोः\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 08: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s, loss=1.771]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Reshuffle cards\n",
            "TARGET: पत्तों को फिर फेंटें\n",
            "PREDICTED: फिर फेंटें फिर फेंटें\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Updated:% s\n",
            "TARGET: अद्यतनीकृत से. \n",
            "PREDICTED: अद्यतनीकृत से .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 09: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s, loss=1.593]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Perl Source File\n",
            "TARGET: पर्ल स्रोत फ़ाइल\n",
            "PREDICTED: पर्ल स्रोत फ़ाइल\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Main Menu\n",
            "TARGET: मुख्य मेन्यू\n",
            "PREDICTED: मुख्य खेलः\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 10: 100%|██████████| 1125/1125 [04:57<00:00,  3.79it/s, loss=1.655]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Place the king of clubs next to the queen of clubs.\n",
            "TARGET: चिड़ी की बेगम के बगल में चिड़ी के वादशाह को रखें. \n",
            "PREDICTED: चिड़ी की बेगम के बगल में चिड़ी के बेगम को रखें .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Build\n",
            "TARGET: बिल्ड\n",
            "PREDICTED: बिल्ड ( _ B )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 11: 100%|██████████| 1125/1125 [04:57<00:00,  3.79it/s, loss=1.503]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Move ~ a onto an empty bottom slot.\n",
            "TARGET: ~ a को एक खाली नीचे स्लॉट में ले जाएँ. \n",
            "PREDICTED: ~ a को एक खाली नीचे स्लॉट में ले जाएँ .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: _ Selection\n",
            "TARGET: चयन (_ S) \n",
            "PREDICTED: चयन ( _ S )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 12: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s, loss=1.687]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Allow temporary spots use\n",
            "TARGET: अस्थायी स्पॉट उपयोग स्वीकारें\n",
            "PREDICTED: अस्थायी स्पॉट उपयोग स्वीकारें\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: NAME\n",
            "TARGET: नाम\n",
            "PREDICTED: नाम\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 13: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s, loss=1.585]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: End of file\n",
            "TARGET: फ़ाइल खोलें\n",
            "PREDICTED: फ़ाइल का नाम\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Component for searching\n",
            "TARGET: खोज रहा है घटक के लिए\n",
            "PREDICTED: खोज रहा है घटक के लिए\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 14: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s, loss=1.414]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Makefile - based project\n",
            "TARGET: मेकफाइल आधारित परियोजना\n",
            "PREDICTED: मेकफाइल आधारित परियोजना\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Restart the game\n",
            "TARGET: वर्तमान खेल पुनःप्रारंभ करें\n",
            "PREDICTED: वर्तमान खेल पुनःप्रारंभ करें\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 15: 100%|██████████| 1125/1125 [04:56<00:00,  3.79it/s, loss=1.572]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Project from Existing Sources\n",
            "TARGET: परियोजना से सोर्सेज़\n",
            "PREDICTED: परियोजना से सोर्सेज़\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Currently activated plugins\n",
            "TARGET: अभी सक्रिय प्लगिन\n",
            "PREDICTED: अभी सक्रिय प्लगिन\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 16: 100%|██████████| 1125/1125 [04:57<00:00,  3.78it/s, loss=1.428]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Add a new file / directory to the CVS tree\n",
            "TARGET: जोड़ें a नया फ़ाइल निर्देशिका को सीवीएस ट्री\n",
            "PREDICTED: जोड़ें a नया निर्देशिका जोड़ें ट्री को सीवीएस\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Serial Line Connection\n",
            "TARGET: सीरियल पंक्ति कनेक्शन\n",
            "PREDICTED: सीरियल पंक्ति कनेक्शन\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 17: 100%|██████████| 1125/1125 [04:56<00:00,  3.79it/s, loss=1.396]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: ace of diamonds\n",
            "TARGET: ईंट का इक्का\n",
            "PREDICTED: ईंट का इक्का\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Field \"% s\" must contains only letters, digits or the following characters \"# $:%% +,. = @ ^ _ `~\". In addition you cannot have a leading dash. Please fix it.\n",
            "TARGET: क्षेत्र से. रखता है अक्षर या अनुसरण कर रहा है अक्षर अन्दर a डैश कृपया. \n",
            "PREDICTED: क्षेत्र से . रखता है अक्षर या अनुसरण कर रहा है अक्षर अन्दर a डैश कृपया .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 18: 100%|██████████| 1125/1125 [04:57<00:00,  3.79it/s, loss=1.343]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Add License Information:\n",
            "TARGET: लाइसेंस जानकारी जोड़ें\n",
            "PREDICTED: लाइसेंस जानकारी जोड़ें\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: _ Select\n",
            "TARGET: चुनें (_ S) \n",
            "PREDICTED: चुनें ( _ S )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 19: 100%|██████████| 1125/1125 [04:56<00:00,  3.79it/s, loss=1.504]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Author Name:\n",
            "TARGET: लेखक नामः\n",
            "PREDICTED: लेखकः\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Remember this selection\n",
            "TARGET: यह चयन याद रखें\n",
            "PREDICTED: यह चयन याद रखें\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def infer_model(config ,src_text:str):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # step 1: Tokenize it\n",
        "    tokenizer_src = get_or_build_tokenizer(config, None, lang = config['lang_src'])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config, None, lang = config['lang_tgt'])\n",
        "    src_token = tokenizer_src.encode(src_text).ids\n",
        "\n",
        "    # create encoder input tensor: (B, seq_len) --> (1, seq_len)\n",
        "    sos_token = torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64)\n",
        "    eos_token = torch.tensor([tokenizer_src.token_to_id('[EOS]')],dtype=torch.int64)\n",
        "    pad_token = torch.tensor([tokenizer_src.token_to_id('[PAD]')],dtype=torch.int64)\n",
        "    encd_num_padding = config['seq_len'] - len(src_token) - 2\n",
        "    encoder_input = torch.cat(\n",
        "        [\n",
        "            sos_token,\n",
        "            torch.tensor(src_token, dtype=torch.int64),\n",
        "            eos_token,\n",
        "            torch.tensor([pad_token]*encd_num_padding, dtype=torch.int64)\n",
        "        ], dim=0\n",
        "    ).unsqueeze(0)\n",
        "    # create encoder input mask # (1,1,seq_len)\n",
        "    encoder_input_mask = (encoder_input!=pad_token).unsqueeze(0).unsqueeze(0).int()\n",
        "    # create model and load the weights\n",
        "    model = get_model(config,tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "    state = torch.load(\"weights/tmodel_17.pt\", map_location=device)\n",
        "    model.load_state_dict(state['model_state_dict'])\n",
        "    model_out = greedy_decode(model,encoder_input.to(device), encoder_input_mask.to(device), tokenizer_src, tokenizer_tgt, 100, device)\n",
        "\n",
        "    # decode model output\n",
        "    tgt_text = tokenizer_tgt.decode(model_out.cpu().numpy())\n",
        "    return tgt_text\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "    config = get_config()\n",
        "    src_text = input(\"Enter: \")\n",
        "    tgt_text = infer_model(config, src_text)\n",
        "    print(f'Translation: {tgt_text}')\n",
        "\n"
      ],
      "metadata": {
        "id": "SeXTTZ2JXSiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbecf011-6e7e-4528-bc2e-622b59788f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter:  Place the king of clubs next to the queen of clubs.\n",
            "Translation: चिड़ी की बेगम के बगल में चिड़ी के वादशाह को रखें .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f81CogZqLGpz",
        "outputId": "9b5a376c-ad0d-4e84-d5fa-270e1f122921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    }
  ]
}